{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is Web Scraping?**  \n",
    "Web scraping is the process of automatically extracting data from websites. It allows users to retrieve large amounts of information efficiently, transforming unstructured data from the web into structured formats suitable for analysis or storage.  \n",
    "\n",
    "#### **Key Features**:  \n",
    "- **Automated Data Extraction**: Eliminates the need for manual copying and pasting.  \n",
    "- **Customizable**: Allows scraping of specific data points as needed.  \n",
    "- **Scalable**: Can handle large datasets across multiple web pages.  \n",
    "\n",
    "#### **Examples of Web Scraping in Action**:  \n",
    "- Collecting product prices from e-commerce websites for price comparison tools.  \n",
    "- Gathering news headlines or articles for sentiment analysis.  \n",
    "- Extracting job postings for recruitment analytics.  \n",
    "- Monitoring stock prices or cryptocurrency trends.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Why is Web Scraping Important in AI?**  \n",
    "Data is the backbone of artificial intelligence and machine learning. Web scraping provides an efficient means to collect diverse and large datasets required for training AI models.  \n",
    "\n",
    "#### **Common AI Applications**:  \n",
    "- **Natural Language Processing (NLP)**: Scrape text data from blogs, news websites, or forums for text generation, sentiment analysis, or summarization.  \n",
    "- **Recommendation Systems**: Gather user reviews, product descriptions, or ratings to create personalized recommendations.  \n",
    "- **Market Analysis**: Extract trends from social media or e-commerce platforms to train predictive models.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ethical and Legal Considerations**  \n",
    "\n",
    "#### **Ethics of Web Scraping**:  \n",
    "Web scraping can impact websites, especially if done irresponsibly. Ethical scraping ensures that:  \n",
    "- The website is not overwhelmed with too many requests in a short time.  \n",
    "- Scraping is done with respect to the website's terms of service.  \n",
    "- Personal and sensitive data is handled responsibly and not misused.\n",
    "\n",
    "#### **Legal Aspects**:  \n",
    "Different countries and jurisdictions have specific laws regarding web scraping.  \n",
    "- **Respect the Robots.txt File**:  \n",
    "  Websites often provide a `robots.txt` file that specifies which pages can or cannot be accessed by web crawlers. Use it as a guide.  \n",
    "- **Terms of Service (ToS)**:  \n",
    "  Many websites explicitly prohibit scraping in their ToS. Violating this can lead to legal consequences.  \n",
    "- **Copyright and Intellectual Property**:  \n",
    "  Avoid using scraped data for purposes that infringe on intellectual property rights.  \n",
    "\n",
    "#### **Real-World Cases**:  \n",
    "- LinkedIn has sued companies for scraping user profiles.  \n",
    "- Courts have ruled both in favor of and against scraping, depending on circumstances.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Tools and Libraries for Web Scraping**  \n",
    "\n",
    "#### **Python Libraries**:  \n",
    "- **Requests**: For sending HTTP requests to fetch web pages.  \n",
    "- **BeautifulSoup**: For parsing HTML and XML documents.  \n",
    "- **Selenium**: For interacting with websites that require JavaScript.  \n",
    "- **Scrapy**: A powerful framework for large-scale web scraping.  \n",
    "\n",
    "#### **Browser Developer Tools**:  \n",
    "- Inspect HTML and CSS structure using Chrome DevTools or Firefox Inspector.  \n",
    "- Identify tags and classes to extract desired data effectively.  \n",
    "\n",
    "#### **APIs as an Alternative**:  \n",
    "Some websites provide APIs, which are easier and more ethical to use for data extraction compared to scraping raw HTML.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Workflow Overview of Web Scraping**  \n",
    "\n",
    "1. **Identify the Target Website**:  \n",
    "   - Decide what data you need and locate the website providing it.  \n",
    "2. **Inspect the Website Structure**:  \n",
    "   - Use browser tools to examine HTML elements and DOM structure.  \n",
    "3. **Write a Scraping Script**:  \n",
    "   - Fetch data using libraries like Requests or Selenium.  \n",
    "   - Parse HTML using BeautifulSoup or equivalent tools.  \n",
    "4. **Extract and Store Data**:  \n",
    "   - Save data in structured formats like CSV, JSON, or databases.  \n",
    "5. **Handle Edge Cases**:  \n",
    "   - Consider rate limits, pagination, or CAPTCHAs.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Potential Challenges in Web Scraping**  \n",
    "\n",
    "#### **Dynamic Content**:  \n",
    "Websites using JavaScript to load data dynamically may require tools like Selenium or Playwright.  \n",
    "\n",
    "#### **Anti-Scraping Measures**:  \n",
    "- Websites may block requests from known bots or implement CAPTCHAs.  \n",
    "- Mitigate using user-agent headers, proxies, or delays between requests.  \n",
    "\n",
    "#### **Unstable Website Structures**:  \n",
    "Frequent updates to the website’s HTML can break scraping scripts.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Understanding HTML Structure**  \n",
    "\n",
    "#### **HTML Basics**  \n",
    "HTML (HyperText Markup Language) is the standard language for creating web pages. Scraping requires familiarity with its structure to identify and extract relevant data.  \n",
    "- **HTML Elements**: Represent different parts of a web page (e.g., headings, paragraphs, links).  \n",
    "  Example:  \n",
    "  ```html\n",
    "  <h1 class=\"title\">Web Scraping Basics</h1>\n",
    "  ```  \n",
    "- **Tags**: Enclosed in angle brackets, like `<h1>` for headings or `<p>` for paragraphs.  \n",
    "- **Attributes**: Provide additional information about elements (e.g., `class`, `id`).  \n",
    "  Example: `<div class=\"content\">...</div>`  \n",
    "\n",
    "#### **Inspecting HTML**  \n",
    "Modern browsers (like Chrome or Firefox) have developer tools to inspect a webpage’s structure.  \n",
    "- Right-click on the page and select **Inspect** or **Inspect Element**.  \n",
    "- Navigate the **Elements Tab** to locate specific HTML components.  \n",
    "\n",
    "#### **DOM (Document Object Model)**  \n",
    "The DOM represents a webpage as a tree structure, where each node corresponds to an element. Scraping libraries like BeautifulSoup interact with this tree to extract content.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Fetching Web Pages**  \n",
    "\n",
    "#### **Using Python’s `requests` Library**  \n",
    "The `requests` library is commonly used to send HTTP requests and fetch web page content.  \n",
    "- Install the library:  \n",
    "  ```bash\n",
    "  pip install requests\n",
    "  ```  \n",
    "- Example:  \n",
    "  ```python\n",
    "  import requests\n",
    "\n",
    "  url = \"https://example.com\"\n",
    "  response = requests.get(url)\n",
    "\n",
    "  if response.status_code == 200:  # Check for a successful response\n",
    "      print(response.text)  # HTML content of the page\n",
    "  ```  \n",
    "\n",
    "#### **Common HTTP Methods**  \n",
    "- **GET**: Retrieve data from a URL (most commonly used).  \n",
    "- **POST**: Send data to the server (useful for form submissions).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Parsing HTML Content**  \n",
    "\n",
    "#### **BeautifulSoup for Parsing**  \n",
    "The BeautifulSoup library helps parse HTML and extract elements.  \n",
    "- Install the library:  \n",
    "  ```bash\n",
    "  pip install beautifulsoup4\n",
    "  ```  \n",
    "- Example:  \n",
    "  ```python\n",
    "  from bs4 import BeautifulSoup\n",
    "\n",
    "  html = \"<html><body><h1 class='title'>Hello, World!</h1></body></html>\"\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "  # Find an element by tag\n",
    "  title = soup.find(\"h1\")\n",
    "  print(title.text)  # Output: Hello, World!\n",
    "  ```  \n",
    "\n",
    "#### **Basic Methods in BeautifulSoup**  \n",
    "- `find(tag, attributes)`: Locate the first occurrence of a tag.  \n",
    "- `find_all(tag, attributes)`: Locate all occurrences of a tag.  \n",
    "- `get(attribute)`: Extract an attribute value (e.g., `href` in `<a>` tags).  \n",
    "- `select(css_selector)`: Use CSS selectors to pinpoint elements.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Extracting Data from HTML**  \n",
    "\n",
    "#### **Targeting Specific Elements**  \n",
    "Use tags, classes, or IDs to locate specific data points.  \n",
    "Example: Scraping all links (`<a>` tags) from a page:  \n",
    "```python\n",
    "html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <a href=\"https://example.com/page1\">Link 1</a>\n",
    "    <a href=\"https://example.com/page2\">Link 2</a>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))\n",
    "```  \n",
    "\n",
    "#### **Using CSS Selectors**  \n",
    "CSS selectors are powerful for extracting nested or complex elements.  \n",
    "Example: Extracting a title with class `header`:  \n",
    "```python\n",
    "soup.select(\"h1.header\")\n",
    "```  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations in Basic Web Scraping**  \n",
    "\n",
    "#### **Respect Website Rules**  \n",
    "- Check the `robots.txt` file:  \n",
    "  Example:  \n",
    "  Navigate to `https://example.com/robots.txt` to see allowed/disallowed URLs.  \n",
    "\n",
    "#### **Handle HTTP Errors**  \n",
    "- Use response codes (e.g., 404 for \"Not Found\", 403 for \"Forbidden\") to handle errors gracefully.  \n",
    "\n",
    "#### **Add Headers to Requests**  \n",
    "- Mimic a browser request by adding headers to avoid being flagged as a bot.  \n",
    "Example:  \n",
    "```python\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "```  \n",
    "\n",
    "#### **Avoid Overloading the Server**  \n",
    "- Add delays between requests using `time.sleep`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Structuring and Storing Scraped Data**\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why is Structuring and Storing Scraped Data Important?**  \n",
    "Once data is scraped, it is typically unstructured and scattered across multiple HTML elements. Structuring and storing the data:  \n",
    "- Enables **efficient analysis**.  \n",
    "- Ensures **scalability** for large datasets.  \n",
    "- Facilitates **interoperability** with other systems or tools.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Common Formats for Structured Data**  \n",
    "\n",
    "#### **1. CSV (Comma-Separated Values)**  \n",
    "- Suitable for tabular data.  \n",
    "- Easily integrates with tools like Excel, Pandas, and database systems.  \n",
    "- Example:  \n",
    "  ```csv\n",
    "  Title,Price,URL\n",
    "  \"Product 1\", \"$10\", \"https://example.com/product1\"\n",
    "  \"Product 2\", \"$20\", \"https://example.com/product2\"\n",
    "  ```\n",
    "\n",
    "#### **2. JSON (JavaScript Object Notation)**  \n",
    "- Ideal for hierarchical or nested data.  \n",
    "- Used extensively in APIs and web applications.  \n",
    "- Example:  \n",
    "  ```json\n",
    "  [\n",
    "      {\"title\": \"Product 1\", \"price\": \"$10\", \"url\": \"https://example.com/product1\"},\n",
    "      {\"title\": \"Product 2\", \"price\": \"$20\", \"url\": \"https://example.com/product2\"}\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "#### **3. Relational Databases (e.g., SQLite, MySQL)**  \n",
    "- Suitable for large datasets requiring complex queries and relationships.  \n",
    "- Enables advanced operations like joins, indexing, and filtering.  \n",
    "\n",
    "#### **4. NoSQL Databases (e.g., MongoDB)**  \n",
    "- Suitable for flexible or hierarchical data structures.  \n",
    "- Popular for JSON-like data storage.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Structuring Data in Python**\n",
    "\n",
    "#### **1. Using Pandas for Tabular Data**  \n",
    "The Pandas library simplifies the process of organizing data into structured tables.  \n",
    "- Install Pandas:  \n",
    "  ```bash\n",
    "  pip install pandas\n",
    "  ```  \n",
    "- Example:  \n",
    "  ```python\n",
    "  import pandas as pd\n",
    "\n",
    "  data = [\n",
    "      {\"title\": \"Product 1\", \"price\": \"$10\", \"url\": \"https://example.com/product1\"},\n",
    "      {\"title\": \"Product 2\", \"price\": \"$20\", \"url\": \"https://example.com/product2\"}\n",
    "  ]\n",
    "  df = pd.DataFrame(data)  # Convert list of dictionaries to DataFrame\n",
    "  print(df)\n",
    "  ```\n",
    "\n",
    "#### **2. Saving Data to a CSV File**  \n",
    "- Example:  \n",
    "  ```python\n",
    "  df.to_csv(\"scraped_data.csv\", index=False)  # Save DataFrame as a CSV file\n",
    "  ```\n",
    "\n",
    "#### **3. Saving Data to JSON**  \n",
    "- Example:  \n",
    "  ```python\n",
    "  df.to_json(\"scraped_data.json\", orient=\"records\")  # Save as JSON\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Storing Data in Databases**\n",
    "\n",
    "#### **1. SQLite (Lightweight Relational Database)**  \n",
    "SQLite is built into Python and is ideal for small to medium-scale projects.  \n",
    "- Install SQLite:  \n",
    "  ```bash\n",
    "  pip install sqlite3\n",
    "  ```  \n",
    "- Example Workflow:  \n",
    "  ```python\n",
    "  import sqlite3\n",
    "\n",
    "  # Connect to SQLite database (creates file if it doesn't exist)\n",
    "  conn = sqlite3.connect(\"scraped_data.db\")\n",
    "  cursor = conn.cursor()\n",
    "\n",
    "  # Create a table\n",
    "  cursor.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS products (\n",
    "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "          title TEXT,\n",
    "          price TEXT,\n",
    "          url TEXT\n",
    "      )\n",
    "  \"\"\")\n",
    "\n",
    "  # Insert data\n",
    "  data = [\n",
    "      (\"Product 1\", \"$10\", \"https://example.com/product1\"),\n",
    "      (\"Product 2\", \"$20\", \"https://example.com/product2\")\n",
    "  ]\n",
    "  cursor.executemany(\"INSERT INTO products (title, price, url) VALUES (?, ?, ?)\", data)\n",
    "\n",
    "  conn.commit()  # Save changes\n",
    "  conn.close()  # Close the connection\n",
    "  ```\n",
    "\n",
    "#### **2. MongoDB (Flexible NoSQL Database)**  \n",
    "MongoDB stores data as JSON-like documents.  \n",
    "- Install MongoDB and PyMongo:  \n",
    "  ```bash\n",
    "  pip install pymongo\n",
    "  ```  \n",
    "- Example Workflow:  \n",
    "  ```python\n",
    "  from pymongo import MongoClient\n",
    "\n",
    "  # Connect to MongoDB\n",
    "  client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "  db = client[\"web_scraping_db\"]\n",
    "  collection = db[\"products\"]\n",
    "\n",
    "  # Insert data\n",
    "  data = [\n",
    "      {\"title\": \"Product 1\", \"price\": \"$10\", \"url\": \"https://example.com/product1\"},\n",
    "      {\"title\": \"Product 2\", \"price\": \"$20\", \"url\": \"https://example.com/product2\"}\n",
    "  ]\n",
    "  collection.insert_many(data)\n",
    "\n",
    "  # Query data\n",
    "  for product in collection.find():\n",
    "      print(product)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Real-World Use Case: Structuring Data for Analysis**  \n",
    "\n",
    "#### **Objective**: Scrape a product listing page and store data for analytics.  \n",
    "**Steps**:  \n",
    "1. Fetch and parse HTML using `requests` and `BeautifulSoup`.  \n",
    "2. Extract relevant fields like product names, prices, and URLs.  \n",
    "3. Organize the data into a Pandas DataFrame.  \n",
    "4. Store the data in both CSV and SQLite formats.  \n",
    "\n",
    "**Code Example**:  \n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Step 1: Fetch the webpage\n",
    "url = \"https://example.com/products\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Extract data\n",
    "products = []\n",
    "for item in soup.find_all(\"div\", class_=\"product\"):\n",
    "    title = item.find(\"h2\").text\n",
    "    price = item.find(\"span\", class_=\"price\").text\n",
    "    link = item.find(\"a\").get(\"href\")\n",
    "    products.append({\"title\": title, \"price\": price, \"url\": link})\n",
    "\n",
    "# Step 3: Store data in Pandas DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Step 4: Save data to CSV\n",
    "df.to_csv(\"products.csv\", index=False)\n",
    "\n",
    "# Step 5: Save data to SQLite\n",
    "conn = sqlite3.connect(\"products.db\")\n",
    "df.to_sql(\"products\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Best Practices for Structuring and Storing Data**  \n",
    "\n",
    "#### **Organizing Data**  \n",
    "- Always label your data fields clearly (e.g., `product_name` vs. `name`).  \n",
    "- Validate and clean the data to remove duplicates or erroneous entries.  \n",
    "\n",
    "#### **Choosing the Right Format**  \n",
    "- Use **CSV** for smaller projects or when working with spreadsheets.  \n",
    "- Use **JSON** for web applications or hierarchical data.  \n",
    "- Use **databases** for large-scale projects requiring complex queries or storage.  \n",
    "\n",
    "#### **Backup and Maintenance**  \n",
    "- Regularly backup your data, especially for large-scale scraping projects.  \n",
    "- Implement a schema or structure to ensure consistency across datasets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Topics in Web Scraping**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Handling Dynamic Content**  \n",
    "Modern websites often use JavaScript to load content dynamically, which cannot be scraped with static HTML parsing tools like `requests` and BeautifulSoup.  \n",
    "\n",
    "#### **Solution: Use a Browser Automation Tool**  \n",
    "**Selenium** is a popular tool for interacting with JavaScript-rendered web pages.  \n",
    "\n",
    "##### **Installing Selenium**:  \n",
    "```bash\n",
    "pip install selenium\n",
    "```  \n",
    "You also need a browser driver (e.g., ChromeDriver) compatible with your browser version.  \n",
    "\n",
    "##### **Example**: Scraping dynamically loaded data  \n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()  # Or the path to your ChromeDriver\n",
    "driver.get(\"https://example.com\")\n",
    "\n",
    "# Wait for content to load and extract it\n",
    "titles = driver.find_elements(By.CLASS_NAME, \"title-class\")\n",
    "for title in titles:\n",
    "    print(title.text)\n",
    "\n",
    "driver.quit()\n",
    "```  \n",
    "\n",
    "---\n",
    "\n",
    "### **Pagination**  \n",
    "#### **Challenge**:  \n",
    "Data is often spread across multiple pages, requiring scripts to navigate between pages.  \n",
    "\n",
    "#### **Solution**: Automate Pagination Handling  \n",
    "- Inspect the \"Next\" button to locate its `href` or `onclick` attribute.  \n",
    "- Iterate through pages programmatically.  \n",
    "\n",
    "##### **Example**: Scraping paginated data  \n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = \"https://example.com/page=\"\n",
    "for page in range(1, 6):  # Scrape the first 5 pages\n",
    "    response = requests.get(base_url + str(page))\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract data from the current page\n",
    "    items = soup.find_all(\"div\", class_=\"item-class\")\n",
    "    for item in items:\n",
    "        print(item.text)\n",
    "```  \n",
    "\n",
    "---\n",
    "\n",
    "### **Avoiding Anti-Scraping Measures**  \n",
    "Websites often implement mechanisms to block scraping.  \n",
    "\n",
    "#### **Common Techniques**:  \n",
    "- **Rate Limiting**: Blocking requests if too many are sent in a short period.  \n",
    "- **CAPTCHAs**: Using challenges to verify human users.  \n",
    "- **IP Blocking**: Denying access to certain IP addresses.  \n",
    "\n",
    "#### **Solutions**:  \n",
    "- **Respect Rate Limits**:  \n",
    "  Introduce delays between requests.  \n",
    "  ```python\n",
    "  import time\n",
    "  time.sleep(2)  # Delay for 2 seconds between requests\n",
    "  ```  \n",
    "\n",
    "- **Use Proxies**:  \n",
    "  Rotate proxies to mimic requests from different IP addresses. Libraries like `scrapy-rotating-proxies` can help.  \n",
    "\n",
    "- **Add Headers**:  \n",
    "  Mimic human behavior by including browser-like headers in requests.  \n",
    "  ```python\n",
    "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "  response = requests.get(url, headers=headers)\n",
    "  ```  \n",
    "\n",
    "- **Solving CAPTCHAs**:  \n",
    "  Use tools like **2Captcha** or **Captcha Solver APIs**, though this should be approached ethically and legally.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Data Cleaning and Storage**  \n",
    "Once data is extracted, it often requires cleaning before analysis.  \n",
    "\n",
    "#### **Cleaning Data**  \n",
    "- Remove special characters and whitespace.  \n",
    "- Handle missing or duplicate values.  \n",
    "- Normalize text formats.  \n",
    "  Example using `pandas`:  \n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  \n",
    "  data = {\"Name\": [\" Alice \", \"Bob\", \"Alice\"], \"Age\": [25, 30, None]}\n",
    "  df = pd.DataFrame(data)\n",
    "  \n",
    "  df[\"Name\"] = df[\"Name\"].str.strip()  # Remove leading/trailing whitespace\n",
    "  df.drop_duplicates(inplace=True)  # Remove duplicates\n",
    "  df.fillna({\"Age\": 0}, inplace=True)  # Fill missing values\n",
    "  print(df)\n",
    "  ```  \n",
    "\n",
    "#### **Storing Data**  \n",
    "- **CSV**: Simple and widely used.  \n",
    "  ```python\n",
    "  df.to_csv(\"output.csv\", index=False)\n",
    "  ```  \n",
    "- **Databases**: Store large datasets using SQLite, PostgreSQL, or MongoDB.  \n",
    "  ```python\n",
    "  import sqlite3\n",
    "  \n",
    "  conn = sqlite3.connect(\"scraping_data.db\")\n",
    "  df.to_sql(\"table_name\", conn, if_exists=\"replace\", index=False)\n",
    "  conn.close()\n",
    "  ```  \n",
    "\n",
    "---\n",
    "\n",
    "### **Working with APIs**  \n",
    "APIs are a cleaner and often preferred alternative to web scraping.  \n",
    "\n",
    "#### **Advantages**:  \n",
    "- Structured data in formats like JSON or XML.  \n",
    "- Reduces the risk of breaking due to website changes.  \n",
    "\n",
    "#### **Example**: Using an API to fetch data  \n",
    "```python\n",
    "import requests\n",
    "\n",
    "api_url = \"https://api.example.com/data\"\n",
    "response = requests.get(api_url, params={\"query\": \"example\"})\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Parse JSON response\n",
    "    print(data)\n",
    "```  \n",
    "\n",
    "---\n",
    "\n",
    "### **Building Scalable Scraping Systems**  \n",
    "\n",
    "#### **Scrapy Framework**  \n",
    "Scrapy is a powerful Python framework for large-scale web scraping.  \n",
    "- Handles requests, parsing, and storage efficiently.  \n",
    "- Includes built-in support for pagination, proxies, and data pipelines.  \n",
    "\n",
    "##### **Installing Scrapy**:  \n",
    "```bash\n",
    "pip install scrapy\n",
    "```  \n",
    "\n",
    "##### **Basic Scrapy Workflow**:  \n",
    "1. Define a spider (a class for scraping data).  \n",
    "2. Use `start_requests` to initiate scraping.  \n",
    "3. Parse the response in the `parse` method.  \n",
    "4. Save data using pipelines.  \n",
    "\n",
    "Example Spider:  \n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = \"example\"\n",
    "    start_urls = [\"https://example.com\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for item in response.css(\"div.item-class\"):\n",
    "            yield {\n",
    "                \"title\": item.css(\"h2::text\").get(),\n",
    "                \"link\": item.css(\"a::attr(href)\").get()\n",
    "            }\n",
    "```  \n",
    "\n",
    "Run the spider:  \n",
    "```bash\n",
    "scrapy crawl example -o output.json\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
